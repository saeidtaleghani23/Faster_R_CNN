{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU torch_snippets\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d sixhky/open-images-bus-trucks -p ./Dataset\n",
    "!unzip ./Dataset/open-images-bus-trucks.zip -d ./Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch_snippets import *\n",
    "from torch_snippets.torch_loader import Report\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.ops import nms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchinfo import summary\n",
    "import torch \n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "# specify the device\n",
    "device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'number of available GPU(s): {torch.cuda.device_count()}')\n",
    "    print(f'current GPU(s): {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img_root = './Dataset/images/images'\n",
    "raw_df_data = pd.read_csv('./Dataset/df.csv')\n",
    "raw_df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the indices corresponding to labels and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2target = {l:t+1 for t,l in enumerate(raw_df_data['LabelName'].unique())}\n",
    "label2target['background'] = 0\n",
    "target2label = {t:l for l,t in label2target.items()}\n",
    "background_class = label2target['background']\n",
    "num_classes = len(label2target) # 3 classes--> 0: background, 1: Bus, 2: Truck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define preprocessing_image function to apply preprocessing on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    This function applies preprocessing on an image and returns a preprocessed image in torch format and on the corresponding device.\n",
    "\n",
    "    Args:\n",
    "        img (array): an input image\n",
    "\n",
    "    Returns:\n",
    "        Tensor: an image with preprocessing applied\n",
    "    \"\"\"\n",
    "    img = torch.tensor(img).permute(2,0,1)\n",
    "    return img.to(device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenDataset(Dataset):\n",
    "    def __init__(self,  df_data_inf = raw_df_data, image_dir ='./Dataset/images/images' ,width=224, hight=224):\n",
    "        \"\"\"\n",
    "        this function intializes the reading necessary information to create a dataset for the training data\n",
    "        Args:\n",
    "            df_data_inf (_type_, optional): metadata of the images. Defaults to raw_df_data.\n",
    "            image_dir (str, optional): directory of available images. Defaults to './Dataset/images/images'.\n",
    "            width (int, optional): width of the image. Defaults to 224.\n",
    "            hight (int, optional): hight of the image. Defaults to 224.\n",
    "        \"\"\"\n",
    "        self.width = width\n",
    "        self.hight = hight\n",
    "        self.image_dir = image_dir\n",
    "        self.images = glob.glob(self.image_dir+'/*')\n",
    "        self.df_data_inf = df_data_inf\n",
    "        self.image_infs = df_data_inf.ImageID.unique()\n",
    "    \n",
    "    # define the __getitem__ method, where we return the preprocessed image and the target value\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves and preprocesses an image and its corresponding target data (bounding boxes and labels)  at the given index for the dataset.\n",
    "    \n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the image and target data to retrieve from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - img (torch.Tensor): The preprocessed image tensor with values normalized between 0 and 1.\n",
    "                - target (dict): A dictionary containing:\n",
    "                    - 'boxes' (torch.Tensor): A tensor of shape (N, 4) representing the ground truth bounding boxes \n",
    "                                            in absolute coordinates. Each box is in the format [xmin, ymin, xmax, ymax].\n",
    "                    - 'labels' (torch.Tensor): A tensor of shape (N,) representing the class labels corresponding to \n",
    "                                            the bounding boxes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract image index\n",
    "        image_id = self.image_infs[idx]\n",
    "        # find the path 2 image\n",
    "        path2image = find(image_id, self.images)\n",
    "        # read the image\n",
    "        img = Image.open(path2image).convert('RGB') # from BGR to RGB\n",
    "        # resize the image and convert its values between 0 and 1\n",
    "        img = np.array(img.resize((self.width, self.hight), resample = Image.BILINEAR))/255.\n",
    "        img_meta_data = self.df_data_inf [self.df_data_inf ['ImageID'] == image_id]\n",
    "        # extract labels in the image and save them as a list\n",
    "        labels =   img_meta_data['LabelName'].values.tolist()    \n",
    "        # extract ground truth bounding boxes coordinates on the image\n",
    "        gt_bbs =   img_meta_data[['XMin', 'YMin', 'XMax', 'YMax']].values\n",
    "        # convert the gt_bbs into absolute values\n",
    "        gt_bbs[:, [0,2]] *= self.width\n",
    "        gt_bbs[:, [1,3]] *= self.hight \n",
    "        # convert the coordinate format into uint and save them as a list\n",
    "        boxes = gt_bbs.astype(np.uint32).tolist()\n",
    "        # torch FRCNN expects the target to contain the absolute coordinates of bounding boxes and the label information \n",
    "        target = {}\n",
    "        target ['boxes'] = torch.Tensor(boxes).float()\n",
    "        target['labels'] = torch.Tensor([label2target[i] for i in labels]).long()      \n",
    "        # apply preprocessing on the image\n",
    "        img = preprocess_image(img)  \n",
    "        return img, target\n",
    "    # define collate_fn method and __len__ method\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to be used in the DataLoader to handle batches of images and targets.\n",
    "\n",
    "        Args:\n",
    "            batch (list): A list of tuples, where each tuple contains:\n",
    "                            - image (torch.Tensor): The image data as a tensor.\n",
    "                            - target (dict): A dictionary containing:\n",
    "                                - 'boxes' (torch.Tensor): Ground truth bounding boxes as (N, 4) tensors.\n",
    "                                - 'labels' (torch.Tensor): Class labels for the bounding boxes.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Two tuples:\n",
    "               - First tuple: A batch of images as tensors.\n",
    "               - Second tuple: A batch of target dictionaries, where each dictionary contains 'boxes' \n",
    "                               and 'labels' as tensors for the corresponding image.\n",
    "        \"\"\"\n",
    "        return tuple(zip(*batch))\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of unique images in the dataset.\n",
    "        This method is used by PyTorch's DataLoader to determine the size of the dataset  and the number of iterations per epoch.\n",
    "    \n",
    "        Returns:\n",
    "        int: The number of unique image IDs in the dataset, corresponding to the total number of data samples available for training or inference.\n",
    "        \"\"\"\n",
    "        return len(self.image_infs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the OpenDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the OpenDataset class\n",
    "test_dataset = OpenDataset(df_data_inf=raw_df_data, image_dir=Img_root, width=224, hight=224)\n",
    "# Test the dataset by getting one sample (e.g., index 0)\n",
    "test_img, test_target = test_dataset.__getitem__(10)\n",
    "# Print the result to check\n",
    "print(f'Image Tensor Shape: {test_img.shape}')\n",
    "print(f'Target: {test_target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(raw_df_data.ImageID.unique(),\n",
    "                                      test_size= 0.1, random_state= 1402)\n",
    "train_df , val_df = raw_df_data [raw_df_data['ImageID'].isin(train_ids)] ,  raw_df_data [raw_df_data['ImageID'].isin(val_ids)] \n",
    "# define train and validation datasets\n",
    "train_dataset = OpenDataset(train_df)\n",
    "val_dataset = OpenDataset(val_df)\n",
    "# define the dataloaders\n",
    "# Note: In PyTorch, the collate_fn is an argument used in the DataLoader class to define how to combine a list of data samples into a single batch. \n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 8, collate_fn = train_dataset.collate_fn, drop_last= True)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size = 8, collate_fn = train_dataset.collate_fn, drop_last= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the DataLoader\n",
    "print(\"Testing DataLoader:\")\n",
    "for batch_idx, (test_image, test_targets) in enumerate(train_dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    # since the image is a tuple, we convert it into tensor \n",
    "    test_image = torch.stack(test_image) \n",
    "    print(f\"Images Shape: {test_image.shape}\")  # Should be (batch_size, 3, height, width)\n",
    "    print(f\"Targets: {test_targets}\")  # Check structure of targets\n",
    "\n",
    "    # Optional: Check the first target in the batch\n",
    "    if test_targets:\n",
    "        print(f\"First Target - Boxes: {test_targets[0]['boxes']}, Labels: {test_targets[0]['labels']}\")\n",
    "    \n",
    "    # Break after the first batch for brevity\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frcnn_model():\n",
    "    \"\"\"\n",
    "    Create and return a Faster R-CNN model with a ResNet-50 backbone and Feature Pyramid Network (FPN).\n",
    "\n",
    "    This function initializes a pre-trained Faster R-CNN model, modifies the classification \n",
    "    head to match the specified number of classes, and returns the updated model.\n",
    "\n",
    "    Returns:\n",
    "        torchvision.models.detection.FasterRCNN: A Faster R-CNN model configured with a \n",
    "        ResNet-50 backbone and a custom box predictor for the specified number of classes.\n",
    "\n",
    "    Note:\n",
    "        The model is pre-trained on the COCO dataset, which helps improve performance on \n",
    "        similar object detection tasks. Ensure that the variable `num_classes` is defined \n",
    "        in the scope before calling this function, as it specifies the number of classes \n",
    "        the model will predict (including the background class if applicable).\n",
    "    \"\"\"\n",
    "    # Initializes a Faster R-CNN model using a ResNet-50 backbone with a Feature Pyramid Network (FPN).\n",
    "    # The argument pretrained=True indicates that the model should be initialized with weights pre-trained on the COCO dataset. \n",
    "    # This helps the model start with useful features learned from a large dataset, improving performance on similar tasks.\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n",
    "    \n",
    "    # Retrieves the number of input features for the classification head of the model.\n",
    "    # model.roi_heads.box_predictor accesses the box predictor component of the model's region of interest (ROI) heads.\n",
    "    # cls_score is the attribute that contains the classification score layer, and in_features gives the number of input features to this layer. \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replaces the existing box predictor of the model with a new FastRCNNPredictor instance.\n",
    "    # FastRCNNPredictor is a class that serves as a predictor for the object detection task, taking the number of input features (in_features) and the number of classes (num_classes) as parameters.\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = get_frcnn_model()\n",
    "summary(test_model, input_size=(1, 3, 224, 224), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(inputs, model, optimizer):\n",
    "    \"\"\"\n",
    "    Trains the model for one batch of input data.\n",
    "\n",
    "    This function performs a forward pass through the model, computes the \n",
    "    losses, performs backpropagation, and updates the model parameters.\n",
    "\n",
    "    Args:\n",
    "        inputs (tuple): A tuple containing two elements:\n",
    "            - input (list): A list of input images.\n",
    "            - targets (list): A list of target dictionaries, each containing the \n",
    "              ground truth for the corresponding input image.\n",
    "        model (torch.nn.Module): The model to be trained.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used to update the \n",
    "            model parameters.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - loss (float): The total loss for the batch, computed as the sum \n",
    "              of individual losses from the model.\n",
    "            - losses (dict): A dictionary containing the individual losses \n",
    "              from the model for various components.\n",
    "\n",
    "    Note:\n",
    "        This function assumes that the model is in training mode. It also \n",
    "        moves the input images and target tensors to the specified device.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    input, targets = inputs\n",
    "    input = list(image.to(device) for image in input)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(input, targets)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, losses\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_batch(inputs, model):\n",
    "    \"\"\"\n",
    "    Validates the model for one batch of input data.\n",
    "\n",
    "    This function performs a forward pass through the model to compute \n",
    "    the losses without updating the model parameters.\n",
    "\n",
    "    Args:\n",
    "        inputs (tuple): A tuple containing two elements:\n",
    "            - input (list): A list of input images.\n",
    "            - targets (list): A list of target dictionaries, each containing the \n",
    "              ground truth for the corresponding input image.\n",
    "        model (torch.nn.Module): The model to be validated.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used to update the \n",
    "            model parameters (not used during validation).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - loss (float): The total loss for the batch, computed as the sum \n",
    "              of individual losses from the model.\n",
    "            - losses (dict): A dictionary containing the individual losses \n",
    "              from the model for various components.\n",
    "\n",
    "    Note:\n",
    "        This function uses the `@torch.no_grad()` decorator to disable \n",
    "        gradient computation, which saves memory and speeds up validation. \n",
    "        The model is set to training mode, which may affect specific \n",
    "        components (like dropout or batch normalization) during validation.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    input, targets = inputs\n",
    "    input = list(image.to(device) for image in input)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(input, targets)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    return loss, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model over increasing epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_frcnn_model().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "n_epochs = 5\n",
    "log = Report(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range (n_epochs):\n",
    "    _n = len(train_dataloader)\n",
    "    for idx, inputs in enumerate(train_dataloader):\n",
    "        loss, losses = train_batch(inputs, model, optimizer)\n",
    "        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = [losses[k] for k in ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']]\n",
    "        pos = (epoch + (idx+1)/_n)\n",
    "        log.record(pos, trn_loss = loss.item(),\n",
    "                   trn_loc_loss = loc_loss.item(),\n",
    "                   trn_reg_loss = regr_loss.item(),\n",
    "                   trn_objectness_loss = loss_objectness.item(),\n",
    "                   trn_rpn_box_reg_loss = loss_rpn_box_reg.item(),\n",
    "                   end = '\\r')\n",
    "    \n",
    "    _n = len(test_dataloader)\n",
    "    for idx,inputs in enumerate(test_dataloader):\n",
    "        loss, losses = valid_batch(inputs, model, optimizer)\n",
    "        oc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = [losses[k] for k in ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']]\n",
    "        pos = (epoch + (idx+1)/_n)\n",
    "        log.record(pos, val_loss = loss.item(),\n",
    "                   val_loc_loss = loc_loss.item(),\n",
    "                   val_reg_loss = regr_loss.item(),\n",
    "                   val_objectness_loss = loss_objectness.item(),\n",
    "                  val_rpn_box_reg_loss = loss_rpn_box_reg.item(),\n",
    "                   end = '\\r')\n",
    "        \n",
    "    if (epoch+1)%(n_epochs//5)==0: log.report_avgs(epoch+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the variation of the various loss values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log.plot_epochs(['trn_loss','val_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-based",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
