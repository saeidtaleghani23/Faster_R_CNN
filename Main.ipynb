{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/sixhky/open-images-bus-trucks\n",
      "License(s): unknown\n",
      "open-images-bus-trucks.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Archive:  ./Dataset/open-images-bus-trucks.zip\n",
      "replace ./Dataset/df.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU torch_snippets\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d sixhky/open-images-bus-trucks -p ./Dataset\n",
    "!unzip ./Dataset/open-images-bus-trucks.zip -d ./Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "number of available GPU(s): 1\n",
      "current GPU(s): NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch_snippets import *\n",
    "from torch_snippets.torch_loader import Report\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.ops import nms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchinfo import summary\n",
    "import torch \n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "# specify the device\n",
    "device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'number of available GPU(s): {torch.cuda.device_count()}')\n",
    "    print(f'current GPU(s): {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>Source</th>\n",
       "      <th>LabelName</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>XMin</th>\n",
       "      <th>XMax</th>\n",
       "      <th>YMin</th>\n",
       "      <th>YMax</th>\n",
       "      <th>IsOccluded</th>\n",
       "      <th>IsTruncated</th>\n",
       "      <th>...</th>\n",
       "      <th>IsDepiction</th>\n",
       "      <th>IsInside</th>\n",
       "      <th>XClick1X</th>\n",
       "      <th>XClick2X</th>\n",
       "      <th>XClick3X</th>\n",
       "      <th>XClick4X</th>\n",
       "      <th>XClick1Y</th>\n",
       "      <th>XClick2Y</th>\n",
       "      <th>XClick3Y</th>\n",
       "      <th>XClick4Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000599864fd15b3</td>\n",
       "      <td>xclick</td>\n",
       "      <td>Bus</td>\n",
       "      <td>1</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.908750</td>\n",
       "      <td>0.156162</td>\n",
       "      <td>0.650047</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.908750</td>\n",
       "      <td>0.156162</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.650047</td>\n",
       "      <td>0.457197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00006bdb1eb5cd74</td>\n",
       "      <td>xclick</td>\n",
       "      <td>Truck</td>\n",
       "      <td>1</td>\n",
       "      <td>0.276667</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>0.141604</td>\n",
       "      <td>0.437343</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.299167</td>\n",
       "      <td>0.276667</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>0.659167</td>\n",
       "      <td>0.141604</td>\n",
       "      <td>0.241855</td>\n",
       "      <td>0.352130</td>\n",
       "      <td>0.437343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00006bdb1eb5cd74</td>\n",
       "      <td>xclick</td>\n",
       "      <td>Truck</td>\n",
       "      <td>1</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>0.999167</td>\n",
       "      <td>0.204261</td>\n",
       "      <td>0.409774</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.849167</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.999167</td>\n",
       "      <td>0.204261</td>\n",
       "      <td>0.398496</td>\n",
       "      <td>0.409774</td>\n",
       "      <td>0.295739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00010bf498b64bab</td>\n",
       "      <td>xclick</td>\n",
       "      <td>Bus</td>\n",
       "      <td>1</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.371250</td>\n",
       "      <td>0.269188</td>\n",
       "      <td>0.705228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.274375</td>\n",
       "      <td>0.371250</td>\n",
       "      <td>0.311875</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.269188</td>\n",
       "      <td>0.493882</td>\n",
       "      <td>0.705228</td>\n",
       "      <td>0.521691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00013f14dd4e168f</td>\n",
       "      <td>xclick</td>\n",
       "      <td>Bus</td>\n",
       "      <td>1</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.194184</td>\n",
       "      <td>0.999062</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.648750</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.194184</td>\n",
       "      <td>0.303940</td>\n",
       "      <td>0.999062</td>\n",
       "      <td>0.523452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ImageID  Source LabelName  Confidence      XMin      XMax  \\\n",
       "0  0000599864fd15b3  xclick       Bus           1  0.343750  0.908750   \n",
       "1  00006bdb1eb5cd74  xclick     Truck           1  0.276667  0.697500   \n",
       "2  00006bdb1eb5cd74  xclick     Truck           1  0.702500  0.999167   \n",
       "3  00010bf498b64bab  xclick       Bus           1  0.156250  0.371250   \n",
       "4  00013f14dd4e168f  xclick       Bus           1  0.287500  0.999375   \n",
       "\n",
       "       YMin      YMax  IsOccluded  IsTruncated  ...  IsDepiction  IsInside  \\\n",
       "0  0.156162  0.650047           1            0  ...            0         0   \n",
       "1  0.141604  0.437343           1            0  ...            0         0   \n",
       "2  0.204261  0.409774           1            1  ...            0         0   \n",
       "3  0.269188  0.705228           0            0  ...            0         0   \n",
       "4  0.194184  0.999062           0            1  ...            0         0   \n",
       "\n",
       "   XClick1X  XClick2X  XClick3X  XClick4X  XClick1Y  XClick2Y  XClick3Y  \\\n",
       "0  0.421875  0.343750  0.795000  0.908750  0.156162  0.512700  0.650047   \n",
       "1  0.299167  0.276667  0.697500  0.659167  0.141604  0.241855  0.352130   \n",
       "2  0.849167  0.702500  0.906667  0.999167  0.204261  0.398496  0.409774   \n",
       "3  0.274375  0.371250  0.311875  0.156250  0.269188  0.493882  0.705228   \n",
       "4  0.920000  0.999375  0.648750  0.287500  0.194184  0.303940  0.999062   \n",
       "\n",
       "   XClick4Y  \n",
       "0  0.457197  \n",
       "1  0.437343  \n",
       "2  0.295739  \n",
       "3  0.521691  \n",
       "4  0.523452  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Img_root = './Dataset/images/images'\n",
    "raw_df_data = pd.read_csv('./Dataset/df.csv')\n",
    "raw_df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the indices corresponding to labels and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2target = {l:t+1 for t,l in enumerate(raw_df_data['LabelName'].unique())}\n",
    "label2target['background'] = 0\n",
    "target2label = {t:l for l,t in label2target.items()}\n",
    "background_class = label2target['background']\n",
    "num_classes = len(label2target) # 3 classes--> 0: background, 1: Bus, 2: Truck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define preprocessing_image function to apply preprocessing on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    This function applies preprocessing on an image and returns a preprocessed image in torch format and on the corresponding device.\n",
    "\n",
    "    Args:\n",
    "        img (array): an input image\n",
    "\n",
    "    Returns:\n",
    "        Tensor: an image with preprocessing applied\n",
    "    \"\"\"\n",
    "    img = torch.tensor(img).permute(2,0,1)\n",
    "    return img.to(device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenDataset(Dataset):\n",
    "    def __init__(self,  df_data_inf = raw_df_data, image_dir ='./Dataset/images/images' ,width=224, hight=224):\n",
    "        \"\"\"\n",
    "        this function intializes the reading necessary information to create a dataset for the training data\n",
    "        Args:\n",
    "            df_data_inf (_type_, optional): metadata of the images. Defaults to raw_df_data.\n",
    "            image_dir (str, optional): directory of available images. Defaults to './Dataset/images/images'.\n",
    "            width (int, optional): width of the image. Defaults to 224.\n",
    "            hight (int, optional): hight of the image. Defaults to 224.\n",
    "        \"\"\"\n",
    "        self.width = width\n",
    "        self.hight = hight\n",
    "        self.image_dir = image_dir\n",
    "        self.images = glob.glob(self.image_dir+'/*')\n",
    "        self.df_data_inf = df_data_inf\n",
    "        self.image_infs = df_data_inf.ImageID.unique()\n",
    "    \n",
    "    # define the __getitem__ method, where we return the preprocessed image and the target value\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves and preprocesses an image and its corresponding target data (bounding boxes and labels)  at the given index for the dataset.\n",
    "    \n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the image and target data to retrieve from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - img (torch.Tensor): The preprocessed image tensor with values normalized between 0 and 1.\n",
    "                - target (dict): A dictionary containing:\n",
    "                    - 'boxes' (torch.Tensor): A tensor of shape (N, 4) representing the ground truth bounding boxes \n",
    "                                            in absolute coordinates. Each box is in the format [xmin, ymin, xmax, ymax].\n",
    "                    - 'labels' (torch.Tensor): A tensor of shape (N,) representing the class labels corresponding to \n",
    "                                            the bounding boxes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract image index\n",
    "        image_id = self.image_infs[idx]\n",
    "        # find the path 2 image\n",
    "        path2image = find(image_id, self.images)\n",
    "        # read the image\n",
    "        img = Image.open(path2image).convert('RGB') # from BGR to RGB\n",
    "        # resize the image and convert its values between 0 and 1\n",
    "        img = np.array(img.resize((self.width, self.hight), resample = Image.BILINEAR))/255.\n",
    "        img_meta_data = self.df_data_inf [self.df_data_inf ['ImageID'] == image_id]\n",
    "        # extract labels in the image and save them as a list\n",
    "        labels =   img_meta_data['LabelName'].values.tolist()    \n",
    "        # extract ground truth bounding boxes coordinates on the image\n",
    "        gt_bbs =   img_meta_data[['XMin', 'YMin', 'XMax', 'YMax']].values\n",
    "        # convert the gt_bbs into absolute values\n",
    "        gt_bbs[:, [0,2]] *= self.width\n",
    "        gt_bbs[:, [1,3]] *= self.hight \n",
    "        # convert the coordinate format into uint and save them as a list\n",
    "        boxes = gt_bbs.astype(np.uint32).tolist()\n",
    "        # torch FRCNN expects the target to contain the absolute coordinates of bounding boxes and the label information \n",
    "        target = {}\n",
    "        target ['boxes'] = torch.Tensor(boxes).float()\n",
    "        target['labels'] = torch.Tensor([label2target[i] for i in labels]).long()      \n",
    "        # apply preprocessing on the image\n",
    "        img = preprocess_image(img)  \n",
    "        return img, target\n",
    "    # define collate_fn method and __len__ method\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to be used in the DataLoader to handle batches of images and targets.\n",
    "\n",
    "        Args:\n",
    "            batch (list): A list of tuples, where each tuple contains:\n",
    "                            - image (torch.Tensor): The image data as a tensor.\n",
    "                            - target (dict): A dictionary containing:\n",
    "                                - 'boxes' (torch.Tensor): Ground truth bounding boxes as (N, 4) tensors.\n",
    "                                - 'labels' (torch.Tensor): Class labels for the bounding boxes.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Two tuples:\n",
    "               - First tuple: A batch of images as tensors.\n",
    "               - Second tuple: A batch of target dictionaries, where each dictionary contains 'boxes' \n",
    "                               and 'labels' as tensors for the corresponding image.\n",
    "        \"\"\"\n",
    "        return tuple(zip(*batch))\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of unique images in the dataset.\n",
    "        This method is used by PyTorch's DataLoader to determine the size of the dataset  and the number of iterations per epoch.\n",
    "    \n",
    "        Returns:\n",
    "        int: The number of unique image IDs in the dataset, corresponding to the total number of data samples available for training or inference.\n",
    "        \"\"\"\n",
    "        return len(self.image_infs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the OpenDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Tensor Shape: torch.Size([3, 224, 224])\n",
      "Target: {'boxes': tensor([[  0.,   0., 223., 223.]]), 'labels': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the OpenDataset class\n",
    "test_dataset = OpenDataset(df_data_inf=raw_df_data, image_dir=Img_root, width=224, hight=224)\n",
    "# Test the dataset by getting one sample (e.g., index 0)\n",
    "test_img, test_target = test_dataset.__getitem__(10)\n",
    "# Print the result to check\n",
    "print(f'Image Tensor Shape: {test_img.shape}')\n",
    "print(f'Target: {test_target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(raw_df_data.ImageID.unique(),\n",
    "                                      test_size= 0.1, random_state= 1402)\n",
    "train_df , val_df = raw_df_data [raw_df_data['ImageID'].isin(train_ids)] ,  raw_df_data [raw_df_data['ImageID'].isin(val_ids)] \n",
    "# define train and validation datasets\n",
    "train_dataset = OpenDataset(train_df)\n",
    "val_dataset = OpenDataset(val_df)\n",
    "# define the dataloaders\n",
    "# Note: In PyTorch, the collate_fn is an argument used in the DataLoader class to define how to combine a list of data samples into a single batch. \n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 8, collate_fn = train_dataset.collate_fn, drop_last= True)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size = 8, collate_fn = train_dataset.collate_fn, drop_last= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader:\n",
      "Batch 1:\n",
      "Images Shape: torch.Size([8, 3, 224, 224])\n",
      "Targets: ({'boxes': tensor([[ 77.,  34., 203., 145.]]), 'labels': tensor([1])}, {'boxes': tensor([[ 61.,  31., 156.,  97.],\n",
      "        [157.,  45., 223.,  91.]]), 'labels': tensor([2, 2])}, {'boxes': tensor([[ 35.,  60.,  83., 157.]]), 'labels': tensor([1])}, {'boxes': tensor([[ 64.,  43., 223., 223.]]), 'labels': tensor([1])}, {'boxes': tensor([[ 13.,  28., 216., 218.]]), 'labels': tensor([2])}, {'boxes': tensor([[ 13.,  11., 220., 210.]]), 'labels': tensor([2])}, {'boxes': tensor([[  7.,  42.,  42.,  71.],\n",
      "        [ 21.,  32., 201., 209.],\n",
      "        [192.,  55., 223.,  87.]]), 'labels': tensor([2, 2, 2])}, {'boxes': tensor([[  0.,  42., 156., 223.]]), 'labels': tensor([2])})\n",
      "First Target - Boxes: tensor([[ 77.,  34., 203., 145.]]), Labels: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# Test the DataLoader\n",
    "print(\"Testing DataLoader:\")\n",
    "for batch_idx, (test_image, test_targets) in enumerate(train_dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    # since the image is a tuple, we convert it into tensor \n",
    "    test_image = torch.stack(test_image) \n",
    "    print(f\"Images Shape: {test_image.shape}\")  # Should be (batch_size, 3, height, width)\n",
    "    print(f\"Targets: {test_targets}\")  # Check structure of targets\n",
    "\n",
    "    # Optional: Check the first target in the batch\n",
    "    if test_targets:\n",
    "        print(f\"First Target - Boxes: {test_targets[0]['boxes']}, Labels: {test_targets[0]['labels']}\")\n",
    "    \n",
    "    # Break after the first batch for brevity\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frcnn_model():\n",
    "    \"\"\"\n",
    "    Create and return a Faster R-CNN model with a ResNet-50 backbone and Feature Pyramid Network (FPN).\n",
    "\n",
    "    This function initializes a pre-trained Faster R-CNN model, modifies the classification \n",
    "    head to match the specified number of classes, and returns the updated model.\n",
    "\n",
    "    Returns:\n",
    "        torchvision.models.detection.FasterRCNN: A Faster R-CNN model configured with a \n",
    "        ResNet-50 backbone and a custom box predictor for the specified number of classes.\n",
    "\n",
    "    Note:\n",
    "        The model is pre-trained on the COCO dataset, which helps improve performance on \n",
    "        similar object detection tasks. Ensure that the variable `num_classes` is defined \n",
    "        in the scope before calling this function, as it specifies the number of classes \n",
    "        the model will predict (including the background class if applicable).\n",
    "    \"\"\"\n",
    "    # Initializes a Faster R-CNN model using a ResNet-50 backbone with a Feature Pyramid Network (FPN).\n",
    "    # The argument pretrained=True indicates that the model should be initialized with weights pre-trained on the COCO dataset. \n",
    "    # This helps the model start with useful features learned from a large dataset, improving performance on similar tasks.\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n",
    "    \n",
    "    # Retrieves the number of input features for the classification head of the model.\n",
    "    # model.roi_heads.box_predictor accesses the box predictor component of the model's region of interest (ROI) heads.\n",
    "    # cls_score is the attribute that contains the classification score layer, and in_features gives the number of input features to this layer. \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replaces the existing box predictor of the model with a new FastRCNNPredictor instance.\n",
    "    # FastRCNNPredictor is a class that serves as a predictor for the object detection task, taking the number of input features (in_features) and the number of classes (num_classes) as parameters.\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeid23/anaconda3/envs/torch-based/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/saeid23/anaconda3/envs/torch-based/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "FasterRCNN                                              [100, 4]                  --\n",
      "├─GeneralizedRCNNTransform: 1-1                         [1, 3, 800, 800]          --\n",
      "├─BackboneWithFPN: 1-2                                  [1, 256, 13, 13]          --\n",
      "│    └─IntermediateLayerGetter: 2-1                     [1, 2048, 25, 25]         --\n",
      "│    │    └─Conv2d: 3-1                                 [1, 64, 400, 400]         (9,408)\n",
      "│    │    └─FrozenBatchNorm2d: 3-2                      [1, 64, 400, 400]         --\n",
      "│    │    └─ReLU: 3-3                                   [1, 64, 400, 400]         --\n",
      "│    │    └─MaxPool2d: 3-4                              [1, 64, 200, 200]         --\n",
      "│    │    └─Sequential: 3-5                             [1, 256, 200, 200]        (212,992)\n",
      "│    │    └─Sequential: 3-6                             [1, 512, 100, 100]        1,212,416\n",
      "│    │    └─Sequential: 3-7                             [1, 1024, 50, 50]         7,077,888\n",
      "│    │    └─Sequential: 3-8                             [1, 2048, 25, 25]         14,942,208\n",
      "│    └─FeaturePyramidNetwork: 2-2                       [1, 256, 13, 13]          --\n",
      "│    │    └─ModuleList: 3-15                            --                        (recursive)\n",
      "│    │    └─ModuleList: 3-16                            --                        (recursive)\n",
      "│    │    └─ModuleList: 3-15                            --                        (recursive)\n",
      "│    │    └─ModuleList: 3-16                            --                        (recursive)\n",
      "│    │    └─ModuleList: 3-15                            --                        (recursive)\n",
      "│    │    └─ModuleList: 3-16                            --                        (recursive)\n",
      "│    │    └─ModuleList: 3-15                            --                        (recursive)\n",
      "│    │    └─ModuleList: 3-16                            --                        (recursive)\n",
      "│    │    └─LastLevelMaxPool: 3-17                      [1, 256, 200, 200]        --\n",
      "├─RegionProposalNetwork: 1-3                            [1000, 4]                 --\n",
      "│    └─RPNHead: 2-3                                     [1, 3, 200, 200]          --\n",
      "│    │    └─Sequential: 3-18                            [1, 256, 200, 200]        590,080\n",
      "│    │    └─Conv2d: 3-19                                [1, 3, 200, 200]          771\n",
      "│    │    └─Conv2d: 3-20                                [1, 12, 200, 200]         3,084\n",
      "│    │    └─Sequential: 3-21                            [1, 256, 100, 100]        (recursive)\n",
      "│    │    └─Conv2d: 3-22                                [1, 3, 100, 100]          (recursive)\n",
      "│    │    └─Conv2d: 3-23                                [1, 12, 100, 100]         (recursive)\n",
      "│    │    └─Sequential: 3-24                            [1, 256, 50, 50]          (recursive)\n",
      "│    │    └─Conv2d: 3-25                                [1, 3, 50, 50]            (recursive)\n",
      "│    │    └─Conv2d: 3-26                                [1, 12, 50, 50]           (recursive)\n",
      "│    │    └─Sequential: 3-27                            [1, 256, 25, 25]          (recursive)\n",
      "│    │    └─Conv2d: 3-28                                [1, 3, 25, 25]            (recursive)\n",
      "│    │    └─Conv2d: 3-29                                [1, 12, 25, 25]           (recursive)\n",
      "│    │    └─Sequential: 3-30                            [1, 256, 13, 13]          (recursive)\n",
      "│    │    └─Conv2d: 3-31                                [1, 3, 13, 13]            (recursive)\n",
      "│    │    └─Conv2d: 3-32                                [1, 12, 13, 13]           (recursive)\n",
      "│    └─AnchorGenerator: 2-4                             [159882, 4]               --\n",
      "├─RoIHeads: 1-4                                         [100, 4]                  --\n",
      "│    └─MultiScaleRoIAlign: 2-5                          [1000, 256, 7, 7]         --\n",
      "│    └─TwoMLPHead: 2-6                                  [1000, 1024]              --\n",
      "│    │    └─Linear: 3-33                                [1000, 1024]              12,846,080\n",
      "│    │    └─Linear: 3-34                                [1000, 1024]              1,049,600\n",
      "│    └─FastRCNNPredictor: 2-7                           [1000, 3]                 --\n",
      "│    │    └─Linear: 3-35                                [1000, 3]                 3,075\n",
      "│    │    └─Linear: 3-36                                [1000, 12]                12,300\n",
      "=========================================================================================================\n",
      "Total params: 41,304,286\n",
      "Trainable params: 41,081,886\n",
      "Non-trainable params: 222,400\n",
      "Total mult-adds (G): 133.97\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 1483.73\n",
      "Params size (MB): 165.22\n",
      "Estimated Total Size (MB): 1649.54\n",
      "=========================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "FasterRCNN                                              [100, 4]                  --\n",
       "├─GeneralizedRCNNTransform: 1-1                         [1, 3, 800, 800]          --\n",
       "├─BackboneWithFPN: 1-2                                  [1, 256, 13, 13]          --\n",
       "│    └─IntermediateLayerGetter: 2-1                     [1, 2048, 25, 25]         --\n",
       "│    │    └─Conv2d: 3-1                                 [1, 64, 400, 400]         (9,408)\n",
       "│    │    └─FrozenBatchNorm2d: 3-2                      [1, 64, 400, 400]         --\n",
       "│    │    └─ReLU: 3-3                                   [1, 64, 400, 400]         --\n",
       "│    │    └─MaxPool2d: 3-4                              [1, 64, 200, 200]         --\n",
       "│    │    └─Sequential: 3-5                             [1, 256, 200, 200]        (212,992)\n",
       "│    │    └─Sequential: 3-6                             [1, 512, 100, 100]        1,212,416\n",
       "│    │    └─Sequential: 3-7                             [1, 1024, 50, 50]         7,077,888\n",
       "│    │    └─Sequential: 3-8                             [1, 2048, 25, 25]         14,942,208\n",
       "│    └─FeaturePyramidNetwork: 2-2                       [1, 256, 13, 13]          --\n",
       "│    │    └─ModuleList: 3-15                            --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                            --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                            --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                            --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                            --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                            --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                            --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                            --                        (recursive)\n",
       "│    │    └─LastLevelMaxPool: 3-17                      [1, 256, 200, 200]        --\n",
       "├─RegionProposalNetwork: 1-3                            [1000, 4]                 --\n",
       "│    └─RPNHead: 2-3                                     [1, 3, 200, 200]          --\n",
       "│    │    └─Sequential: 3-18                            [1, 256, 200, 200]        590,080\n",
       "│    │    └─Conv2d: 3-19                                [1, 3, 200, 200]          771\n",
       "│    │    └─Conv2d: 3-20                                [1, 12, 200, 200]         3,084\n",
       "│    │    └─Sequential: 3-21                            [1, 256, 100, 100]        (recursive)\n",
       "│    │    └─Conv2d: 3-22                                [1, 3, 100, 100]          (recursive)\n",
       "│    │    └─Conv2d: 3-23                                [1, 12, 100, 100]         (recursive)\n",
       "│    │    └─Sequential: 3-24                            [1, 256, 50, 50]          (recursive)\n",
       "│    │    └─Conv2d: 3-25                                [1, 3, 50, 50]            (recursive)\n",
       "│    │    └─Conv2d: 3-26                                [1, 12, 50, 50]           (recursive)\n",
       "│    │    └─Sequential: 3-27                            [1, 256, 25, 25]          (recursive)\n",
       "│    │    └─Conv2d: 3-28                                [1, 3, 25, 25]            (recursive)\n",
       "│    │    └─Conv2d: 3-29                                [1, 12, 25, 25]           (recursive)\n",
       "│    │    └─Sequential: 3-30                            [1, 256, 13, 13]          (recursive)\n",
       "│    │    └─Conv2d: 3-31                                [1, 3, 13, 13]            (recursive)\n",
       "│    │    └─Conv2d: 3-32                                [1, 12, 13, 13]           (recursive)\n",
       "│    └─AnchorGenerator: 2-4                             [159882, 4]               --\n",
       "├─RoIHeads: 1-4                                         [100, 4]                  --\n",
       "│    └─MultiScaleRoIAlign: 2-5                          [1000, 256, 7, 7]         --\n",
       "│    └─TwoMLPHead: 2-6                                  [1000, 1024]              --\n",
       "│    │    └─Linear: 3-33                                [1000, 1024]              12,846,080\n",
       "│    │    └─Linear: 3-34                                [1000, 1024]              1,049,600\n",
       "│    └─FastRCNNPredictor: 2-7                           [1000, 3]                 --\n",
       "│    │    └─Linear: 3-35                                [1000, 3]                 3,075\n",
       "│    │    └─Linear: 3-36                                [1000, 12]                12,300\n",
       "=========================================================================================================\n",
       "Total params: 41,304,286\n",
       "Trainable params: 41,081,886\n",
       "Non-trainable params: 222,400\n",
       "Total mult-adds (G): 133.97\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 1483.73\n",
       "Params size (MB): 165.22\n",
       "Estimated Total Size (MB): 1649.54\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = get_frcnn_model()\n",
    "summary(test_model, input_size=(1, 3, 224, 224), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(inputs, model, optimizer):\n",
    "    \"\"\"\n",
    "    Trains the model for one batch of input data.\n",
    "\n",
    "    This function performs a forward pass through the model, computes the \n",
    "    losses, performs backpropagation, and updates the model parameters.\n",
    "\n",
    "    Args:\n",
    "        inputs (tuple): A tuple containing two elements:\n",
    "            - input (list): A list of input images.\n",
    "            - targets (list): A list of target dictionaries, each containing the \n",
    "              ground truth for the corresponding input image.\n",
    "        model (torch.nn.Module): The model to be trained.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used to update the \n",
    "            model parameters.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - loss (float): The total loss for the batch, computed as the sum \n",
    "              of individual losses from the model.\n",
    "            - losses (dict): A dictionary containing the individual losses \n",
    "              from the model for various components.\n",
    "\n",
    "    Note:\n",
    "        This function assumes that the model is in training mode. It also \n",
    "        moves the input images and target tensors to the specified device.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    input, targets = inputs\n",
    "    input = list(image.to(device) for image in input)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(input, targets)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, losses\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_batch(inputs, model):\n",
    "    \"\"\"\n",
    "    Validates the model for one batch of input data.\n",
    "\n",
    "    This function performs a forward pass through the model to compute \n",
    "    the losses without updating the model parameters.\n",
    "\n",
    "    Args:\n",
    "        inputs (tuple): A tuple containing two elements:\n",
    "            - input (list): A list of input images.\n",
    "            - targets (list): A list of target dictionaries, each containing the \n",
    "              ground truth for the corresponding input image.\n",
    "        model (torch.nn.Module): The model to be validated.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used to update the \n",
    "            model parameters (not used during validation).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - loss (float): The total loss for the batch, computed as the sum \n",
    "              of individual losses from the model.\n",
    "            - losses (dict): A dictionary containing the individual losses \n",
    "              from the model for various components.\n",
    "\n",
    "    Note:\n",
    "        This function uses the `@torch.no_grad()` decorator to disable \n",
    "        gradient computation, which saves memory and speeds up validation. \n",
    "        The model is set to training mode, which may affect specific \n",
    "        components (like dropout or batch normalization) during validation.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    input, targets = inputs\n",
    "    input = list(image.to(device) for image in input)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(input, targets)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    return loss, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model over increasing epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_frcnn_model().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "n_epochs = 5\n",
    "log = Report(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1.000  trn_loss: 0.076  trn_loc_loss: 0.024  trn_reg_loss: 0.039  trn_objectness_loss: 0.007  trn_rpn_box_reg_loss: 0.005  (1912.38s - 7649.54s remaining)"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "valid_batch() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m _n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_dataloader)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dataloader):\n\u001b[0;32m---> 16\u001b[0m     loss, losses \u001b[38;5;241m=\u001b[39m \u001b[43mvalid_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     oc_loss, regr_loss, loss_objectness, loss_rpn_box_reg \u001b[38;5;241m=\u001b[39m [losses[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_classifier\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_box_reg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_objectness\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_rpn_box_reg\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     18\u001b[0m     pos \u001b[38;5;241m=\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m (idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m_n)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-based/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: valid_batch() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "for epoch in range (n_epochs):\n",
    "    _n = len(train_dataloader)\n",
    "    for idx, inputs in enumerate(train_dataloader):\n",
    "        loss, losses = train_batch(inputs, model, optimizer)\n",
    "        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = [losses[k] for k in ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']]\n",
    "        pos = (epoch + (idx+1)/_n)\n",
    "        log.record(pos, trn_loss = loss.item(),\n",
    "                   trn_loc_loss = loc_loss.item(),\n",
    "                   trn_reg_loss = regr_loss.item(),\n",
    "                   trn_objectness_loss = loss_objectness.item(),\n",
    "                   trn_rpn_box_reg_loss = loss_rpn_box_reg.item(),\n",
    "                   end = '\\r')\n",
    "    \n",
    "    _n = len(test_dataloader)\n",
    "    for idx,inputs in enumerate(test_dataloader):\n",
    "        loss, losses = valid_batch(inputs, model,)\n",
    "        oc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = [losses[k] for k in ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']]\n",
    "        pos = (epoch + (idx+1)/_n)\n",
    "        log.record(pos, val_loss = loss.item(),\n",
    "                   val_loc_loss = loc_loss.item(),\n",
    "                   val_reg_loss = regr_loss.item(),\n",
    "                   val_objectness_loss = loss_objectness.item(),\n",
    "                  val_rpn_box_reg_loss = loss_rpn_box_reg.item(),\n",
    "                   end = '\\r')\n",
    "        \n",
    "    if (epoch+1)%(n_epochs//5)==0: log.report_avgs(epoch+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the variation of the various loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.plot_epochs(['trn_loss','val_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-based",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
